<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SUSE Edge Documentation | Automated Provisioning with ZTP</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Automated Provisioning with ZTP"/>
<meta name="description" content="The downstream cluster provisioning with ZTP (Zero Touch Provisioning) is a feature that allows you to automate the provisioning of downstream cluste…"/>
<meta name="book-title" content="SUSE Edge Documentation"/>
<meta name="chapter-title" content="Chapter 26. Automated Provisioning with ZTP"/>
<meta name="tracker-url" content="https://github.com/suse-edge/suse-edge.github.io/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Automated Provisioning with ZTP"/>
<meta property="og:description" content="The downstream cluster provisioning with ZTP (Zero Touch Pr…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Automated Provisioning with ZTP"/>
<meta name="twitter:description" content="The downstream cluster provisioning with ZTP (Zero Touch Pr…"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
    "inLanguage": "en",
    

    "headline": "Automated Provisioning with ZTP",
  
    "description": "Automated Provisioning with ZTP",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      
    "dateModified": "2024-04-22T00:00+02:00",
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="atip-features.html" title="Chapter 25. Telco Features Configuration"/><link rel="next" href="atip-lifecycle.html" title="Chapter 27. Lifecycle Actions"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="wide offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">SUSE Edge Documentation</a><span> / </span><a class="crumb" href="id-product-documentation.html">Product Documentation</a><span> / </span><a class="crumb" href="atip-automated-provisioning.html">Automated Provisioning with ZTP</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">SUSE Edge Documentation</div><ol><li><a href="id-suse-edge-documentation.html" class=" "><span class="title-number"> </span><span class="title-name">SUSE Edge Documentation</span></a></li><li><a href="id-quickstarts.html" class="has-children "><span class="title-number">I </span><span class="title-name">Quickstarts</span></a><ol><li><a href="quickstart-metal3.html" class=" "><span class="title-number">1 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></a></li><li><a href="quickstart-elemental.html" class=" "><span class="title-number">2 </span><span class="title-name">Remote host onboarding with Elemental</span></a></li><li><a href="quickstart-eib.html" class=" "><span class="title-number">3 </span><span class="title-name">Standalone Clusters with Edge Image Builder</span></a></li></ol></li><li><a href="id-components-used-2.html" class="has-children "><span class="title-number">II </span><span class="title-name">Components Used</span></a><ol><li><a href="components-rancher.html" class=" "><span class="title-number">4 </span><span class="title-name">Rancher</span></a></li><li><a href="components-fleet.html" class=" "><span class="title-number">5 </span><span class="title-name">Fleet</span></a></li><li><a href="components-slmicro.html" class=" "><span class="title-number">6 </span><span class="title-name">SLE Micro</span></a></li><li><a href="components-metal3.html" class=" "><span class="title-number">7 </span><span class="title-name">Metal³</span></a></li><li><a href="components-eib.html" class=" "><span class="title-number">8 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="components-elemental.html" class=" "><span class="title-number">9 </span><span class="title-name">Elemental</span></a></li><li><a href="components-k3s.html" class=" "><span class="title-number">10 </span><span class="title-name">K3s</span></a></li><li><a href="components-rke2.html" class=" "><span class="title-number">11 </span><span class="title-name">RKE2</span></a></li><li><a href="components-longhorn.html" class=" "><span class="title-number">12 </span><span class="title-name">Longhorn</span></a></li><li><a href="components-neuvector.html" class=" "><span class="title-number">13 </span><span class="title-name">NeuVector</span></a></li><li><a href="components-metallb.html" class=" "><span class="title-number">14 </span><span class="title-name">MetalLB</span></a></li><li><a href="components-kubevirt.html" class=" "><span class="title-number">15 </span><span class="title-name">Edge Virtualization</span></a></li></ol></li><li><a href="id-how-to-guides.html" class="has-children "><span class="title-number">III </span><span class="title-name">How To Guides</span></a><ol><li><a href="guides-metallb-k3s.html" class=" "><span class="title-number">16 </span><span class="title-name">MetalLB on K3s (using L2)</span></a></li><li><a href="guides-metallb-kubernetes.html" class=" "><span class="title-number">17 </span><span class="title-name">MetalLB in front of the Kubernetes API server</span></a></li></ol></li><li><a href="id-third-party-integration.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Third Party Integration</span></a><ol><li><a href="integrations-linkert.html" class=" "><span class="title-number">18 </span><span class="title-name">Using the Linkerd Service Mesh</span></a></li><li><a href="integrations-nats.html" class=" "><span class="title-number">19 </span><span class="title-name">NATS</span></a></li><li><a href="id-nvidia-gpus-on-sle-micro.html" class=" "><span class="title-number">20 </span><span class="title-name">NVIDIA GPU’s on SLE Micro</span></a></li></ol></li><li class="active"><a href="id-product-documentation.html" class="has-children you-are-here"><span class="title-number">V </span><span class="title-name">Product Documentation</span></a><ol><li><a href="atip.html" class=" "><span class="title-number">21 </span><span class="title-name">SUSE Adaptive Telco Infrastructure Platform (ATIP)</span></a></li><li><a href="atip-architecture.html" class=" "><span class="title-number">22 </span><span class="title-name">Concept &amp; Architecture</span></a></li><li><a href="atip-requirements.html" class=" "><span class="title-number">23 </span><span class="title-name">Requirements &amp; Assumptions</span></a></li><li><a href="atip-management-cluster.html" class=" "><span class="title-number">24 </span><span class="title-name">Setting up the Management Cluster</span></a></li><li><a href="atip-features.html" class=" "><span class="title-number">25 </span><span class="title-name">Telco Features Configuration</span></a></li><li><a href="atip-automated-provisioning.html" class=" you-are-here"><span class="title-number">26 </span><span class="title-name">Automated Provisioning with ZTP</span></a></li><li><a href="atip-lifecycle.html" class=" "><span class="title-number">27 </span><span class="title-name">Lifecycle Actions</span></a></li></ol></li><li><a href="id-appendix.html" class="has-children "><span class="title-number">A </span><span class="title-name">Appendix</span></a><ol><li><a href="id-appendix.html#id-terminology" class=" "><span class="title-number"> </span><span class="title-name">Terminology</span></a></li></ol></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="atip-automated-provisioning" data-id-title="Automated Provisioning with ZTP"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">26 </span><span class="title-name">Automated Provisioning with ZTP</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect1" id="id-introduction-2" data-id-title="Introduction"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.1 </span><span class="title-name">Introduction</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-introduction-2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The downstream cluster provisioning with <code class="literal">ZTP</code> (Zero Touch Provisioning) is a feature that allows you to automate the provisioning of downstream clusters. This feature is useful when you have a large number of downstream clusters to provision, and you want to automate the process.
From a technical point of view, the management cluster contains the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">SUSE Linux Enterprise Micro RT</code> as the OS. Depending on the use case, some configurations like networking, storage, users and kernel arguments can be customized.</p></li><li class="listitem"><p><code class="literal">RKE2</code> as the Kubernetes cluster. The default <code class="literal">CNI</code> plugin is <code class="literal">Cilium</code>.  Depending on the use case, some <code class="literal">CNI</code> plugins can be used, such as <code class="literal">Cilium+Multus</code>.</p></li><li class="listitem"><p><code class="literal">Longhorn</code> as the storage solution.</p></li><li class="listitem"><p><code class="literal">NeuVector</code> as the security solution.</p></li></ul></div><p>Depending on the use case, <code class="literal">MetalLB</code> can be used as the load balancer for the Kubernetes cluster to enable high availability on multi-node clusters.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">MetalLB</code> as the component to deploy multi-node clusters using a load balancer.</p></li></ul></div><div id="id-1.7.8.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about <code class="literal">SUSE Linux Enterprise Micro</code>, please check: <a class="xref" href="components-slmicro.html" title="Chapter 6. SLE Micro">Chapter 6, <em>SLE Micro</em></a></p><p>For more information about <code class="literal">RKE2</code>, please check: <a class="xref" href="components-rke2.html" title="Chapter 11. RKE2">Chapter 11, <em>RKE2</em></a></p><p>For more information about <code class="literal">Longhorn</code>, please check: <a class="xref" href="components-longhorn.html" title="Chapter 12. Longhorn">Chapter 12, <em>Longhorn</em></a></p><p>For more information about <code class="literal">NeuVector</code>, please check: <a class="xref" href="components-neuvector.html" title="Chapter 13. NeuVector">Chapter 13, <em>NeuVector</em></a></p></div><p>The <code class="literal">ZTP</code> workflow allows to automate the provisioning of the downstream clusters. The following sections describe the different <code class="literal">ZTP</code> workflows and some additional features that can be added to the provisioning process:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#ztp-eib-edge-image" title="26.2. Generate the downstream Image using EIB">Section 26.2, “Generate the downstream Image using EIB”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#ztp-single-node" title="26.3. Downstream Cluster Provisioning with ZTP (single-node)">Section 26.3, “Downstream Cluster Provisioning with ZTP (single-node)”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#ztp-multi-node" title="26.4. Downstream Cluster Provisioning with ZTP (multi-node)">Section 26.4, “Downstream Cluster Provisioning with ZTP (multi-node)”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#ztp-add-advanced-network" title="26.5. Additional Features with ZTP - advanced network configuration">Section 26.5, “Additional Features with ZTP - advanced network configuration”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#ztp-add-telco" title="26.6. Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)">Section 26.6, “Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#ztp-private-registry" title="26.7. Additional Features with ZTP - Private Registry">Section 26.7, “Additional Features with ZTP - Private Registry”</a></p></li></ul></div></section><section class="sect1" id="ztp-eib-edge-image" data-id-title="Generate the downstream Image using EIB"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.2 </span><span class="title-name">Generate the downstream Image using EIB</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#ztp-eib-edge-image">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Using <code class="literal">Edge Image Builder</code> to create the image to be used in the downstream clusters, a lot of configurations can be customized, but in this guide, we will cover the minimal configurations necessary to set up the downstream cluster.
Edge Image Builder is typically run from inside a container so, if you don’t already have a way to run containers, we need to start by installing a container runtime such as <a class="link" href="https://podman.io" target="_blank">Podman</a> or <a class="link" href="https://rancherdesktop.io" target="_blank">Rancher Desktop</a>. For this document, we will assume you already have a container runtime available.</p><p><span class="strong"><strong>Directory Structure</strong></span></p><p>When running the <code class="literal">EIB</code>, a directory will be mounted from the host, so the first thing to do is to create a directory structure to be used by the <code class="literal">EIB</code> to store the configuration files and the image itself.
This directory has the following structure:</p><div class="verbatim-wrap"><pre class="screen">├── downstream-cluster-config.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ growfs.sh</pre></div><p>Where the <code class="literal">network</code> folder is optional, just in case you need to customize for DHCP-less, or advanced networking scenarios (covered in the following sections).</p><div id="id-1.7.8.3.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The image <code class="literal">SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw</code> has to be downloaded from the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a> or the <a class="link" href="https://www.suse.com/download/sle-micro/" target="_blank">SUSE Download page</a>, and it has to be located under the <code class="literal">base-images</code> folder.</p></div><p><span class="strong"><strong>Downstream Cluster config file</strong></span></p><p>The <code class="literal">downstream-cluster-config.yaml</code> file is the main configuration file for the downstream cluster.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Depending on the use case, the common configuration file for all scenarios contains the following information:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
  outputImageName: eibimage-slemicro55rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${USERKEY1}</pre></div><p>where the <code class="literal">${ROOT_PASSWORD}</code> is the encrypted password for the root user. Just for testing purposes, if you want to generate the encrypted password, you can use the following command:</p><div class="verbatim-wrap"><pre class="screen">openssl passwd -6 PASSWORD</pre></div><p>For production environment, the recommendation is to use the ssh-keys which can be added to the users block replacing the <code class="literal">${USERKEY1}</code> with the real ssh keys.</p><div class="itemizedlist" id="ztp-add-telco-feature-eib"><ul class="itemizedlist"><li class="listitem"><p>In case you need to configure some Telco features like <code class="literal">dpdk</code>, <code class="literal">sr-iov</code> or <code class="literal">FEC</code>, the following file includes the packages required to enable these features:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
  outputImageName: eibimage-slemicro55rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${user1Key1}
  packages:
    packageList:
      - jq
      - dpdk22
      - dpdk22-tools
      - libdpdk-23
      - pf-bb-config
    additionalRepos:
      - url: https://download.opensuse.org/repositories/isv:/SUSE:/Edge:/Telco/SLEMicro5.5/
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</pre></div><p>Where the <code class="literal">${SCC_REGISTRATION_CODE}</code> is the registration code for the SUSE Customer Center, and the package list contains the minimum packages to be used for the Telco profiles.
In case you want to use the <code class="literal">pf-bb-config</code> package (to enable the <code class="literal">FEC</code> feature and binding with drivers), the <code class="literal">additionalRepos</code> block has to be included to add the <code class="literal">SUSE Edge Telco</code> repository.</p><div class="itemizedlist" id="ztp-add-network-eib"><ul class="itemizedlist"><li class="listitem"><p>In case you need to configure the networking for a DHCP-less or more networking advanced scenarios, the following folder and file should be included with the networking configuration:</p></li></ul></div><p>In the <code class="literal">network</code> folder, the <code class="literal">configure-network.sh</code> file contains the networking configuration for the downstream cluster.
The following script tries to statically configure a NIC when the bare metal object has been created with a secret containing the static network configuration covered in the Additional Features with ZTP - DHCP Less (<a class="xref" href="atip-automated-provisioning.html#ztp-add-advanced-network" title="26.5. Additional Features with ZTP - advanced network configuration">Section 26.5, “Additional Features with ZTP - advanced network configuration”</a>) section.
Also, it uses the networking information to generate the network configuration for the downstream cluster using the <code class="literal">nmc</code> <a class="link" href="https://github.com/suse-edge/nm-configurator" target="_blank">tool</a>.</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/${DESIRED_HOSTNAME}.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</pre></div><p>In the <code class="literal">network</code> folder, the <code class="literal">configure-network.sh</code> file contains the networking configuration for the downstream cluster.
The following script tries to statically configure a NIC in the case the bare metal object has been created with a secret containing the static network configuration covered in the Additional Features with ZTP - advanced network configuration (<a class="xref" href="atip-automated-provisioning.html#ztp-add-advanced-network" title="26.5. Additional Features with ZTP - advanced network configuration">Section 26.5, “Additional Features with ZTP - advanced network configuration”</a>) section.
Also, it uses the networking information to generate the network configuration for the downstream cluster using the <code class="literal">nmc</code> <a class="link" href="https://github.com/suse-edge/nm-configurator" target="_blank">tool</a>.</p><div class="itemizedlist" id="ztp-add-custom-script-growfs"><ul class="itemizedlist"><li class="listitem"><p>There is a custom script (<code class="literal">custom/scripts/growfs.sh</code>) which is required to grow the filesystem to the disk size once it’s installed during the process. The <code class="literal">growfs.sh</code> script contains the following information:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</pre></div><div id="id-1.7.8.3.24" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>You can add your own custom scripts to be executed during the provisioning process.
For more information please check out: <a class="xref" href="components-eib.html" title="Chapter 8. Edge Image Builder">Chapter 8, <em>Edge Image Builder</em></a></p></div><p><span class="strong"><strong>Image Creation</strong></span></p><p>Once the directory structure is prepared following the previous sections, run the following command to build the image:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --privileged -it -v $PWD/eib/:/eib \
 registry.opensuse.org/isv/suse/edge/edgeimagebuilder/containerfile/suse/edge-image-builder:1.0.0 \
 --config-file downstream-cluster-config.yaml --config-dir /eib --build-dir /eib/_build</pre></div><p>This will create the output ISO image file, named <code class="literal">eibimage-slemicro55rt-telco.raw</code>, based on the definition described above.</p></section><section class="sect1" id="ztp-single-node" data-id-title="Downstream Cluster Provisioning with ZTP (single-node)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.3 </span><span class="title-name">Downstream Cluster Provisioning with ZTP (single-node)</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#ztp-single-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section describes the workflow to automate the provisioning of a single-node downstream cluster using <code class="literal">ZTP</code>.
Basically, this is the simplest way to automate the provisioning of a downstream cluster.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> as described in the previous section (<a class="xref" href="atip-automated-provisioning.html#ztp-eib-edge-image" title="26.2. Generate the downstream Image using EIB">Section 26.2, “Generate the downstream Image using EIB”</a>) with the minimal configuration to set up the downstream cluster has to be located in the management cluster exactly on the path you configured on this section (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>)</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. Please check the  Management Cluster section for more information. <a class="xref" href="atip-management-cluster.html" title="Chapter 24. Setting up the Management Cluster">Chapter 24, <em>Setting up the Management Cluster</em></a></p></li></ul></div><p><span class="strong"><strong>Workflow</strong></span></p><p>The following diagram shows the workflow to automate the provisioning of a single-node downstream cluster using <code class="literal">ZTP</code>:</p><div class="informalfigure"><div class="mediaobject"><a href="images/atip-automated-singlenode1.png"><img src="images/atip-automated-singlenode1.png" width="NaN" alt="atip automated singlenode1" title="atip automated singlenode1"/></a></div></div><p>There are two different steps to automate the provisioning of a single-node downstream cluster using <code class="literal">ZTP</code>:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Enroll the bare metal Host to make it available for the provisioning process.</p></li><li class="listitem"><p>Provision the bare metal Host to install and configure the operating system and the Kubernetes cluster.</p></li></ol></div><p><span class="strong"><strong>Enroll the bare metal Host</strong></span></p><p>The first step is to enroll the new bare metal host in the management cluster to make it available to be provisioned.
To do that, the following file (<code class="literal">bmh-example.yaml</code>) has to be created in the management-cluster, to specify the <code class="literal">BMC</code> credentials to be used and the <code class="literal">BaremetalHost</code> object to be enrolled:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: flexran-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</pre></div><p>where:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${BMC_USERNAME}</code> - The username for the <code class="literal">BMC</code> of the new bare metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_PASSWORD}</code> - The password for the <code class="literal">BMC</code> of the new bare metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_MAC}</code> - The <code class="literal">MAC</code> address of the new bare metal host to be used.</p></li><li class="listitem"><p><code class="literal">${BMC_ADDRESS}</code> - The <code class="literal">URL</code> for the bare metal host <code class="literal">BMC</code> (e.g <code class="literal">redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</code>). If you want to know more about the different options available depending on your hardware provider, please check the following <a class="link" href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md" target="_blank">link</a>.</p></li></ul></div><p>Once the file is created, the following command has to be executed in the management cluster to start enrolling the new bare metal host in the management cluster:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f bmh-example.yaml</pre></div><p>The new bare metal host object will be enrolled changing its state from registering to inspecting and available. The changes can be checked using the following command:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get bmh</pre></div><div id="id-1.7.8.4.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">BaremetalHost</code> object will be in the <code class="literal">registering</code> state until the <code class="literal">BMC</code> credentials are validated. Once the credentials are validated, the <code class="literal">BaremetalHost</code> object will change its state to <code class="literal">inspecting</code>, and this step could take some time depending on the hardware (up to 20 minutes). During the inspecting phase, the hardware information is retrieved and the Kubernetes object is updated. You can check the information using the following command <code class="literal">kubectl get bmh -o yaml</code>.</p></div><p id="ztp-single-node-provision"><span class="strong"><strong>Provision Step</strong></span></p><p>Once the bare metal host is enrolled and available, the next step is to provision the bare metal host to install and configure the operating system and the Kubernetes cluster.
To do that, the following file (<code class="literal">capi-provisioning-example.yaml</code>) has to be created in the management-cluster with the following information (the <code class="literal">capi-provisioning-example.yaml</code> can be generated joining the following blocks).</p><div id="id-1.7.8.4.22" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Only the values between <code class="literal">$\{…​\}</code> have to be replaced with the real values.</p></div><p>The following block is the cluster definition where the networking can be configured using the <code class="literal">pods</code> and the <code class="literal">services</code> blocks. Also, it contains the references to the control plane and the infrastructure (using the <code class="literal">Metal<sup>3</sup></code> provider) objects to be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</pre></div><p>The <code class="literal">Metal3Cluster</code> object to specify the control plane endpoint (replacing the <code class="literal">${DOWNSTREAM_CONTROL_PLANE_IP}</code>) to be configured and the <code class="literal">noCloudProvider</code> because a bare metal node will be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IP}
    port: 6443
  noCloudProvider: true</pre></div><p>The <code class="literal">RKE2ControlPlane</code> object to specify the control plane configuration to be used and the <code class="literal">Metal3MachineTemplate</code> object to specify the control plane image to be used.
Also, it contains the information about the number of replicas to be used (in this case, one) and the <code class="literal">CNI</code> plugin to be used (in this case, <code class="literal">Cilium</code>).
The agentConfig block contains the <code class="literal">Ignition</code> format to be used and the <code class="literal">additionalUserData</code> to be used to configure the <code class="literal">RKE2</code> node with some information like a systemd named <code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.
The last block of information contains the Kubernetes version to be used. The <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (e.g. <code class="literal">v1.28.7+rke2r1</code>).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</pre></div><p>The <code class="literal">Metal3MachineTemplate</code> object to specify the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">dataTemplate</code> to be used doing a reference to the template.</p></li><li class="listitem"><p>The <code class="literal">hostSelector</code> to be used matching with the label created during the enrollment process.</p></li><li class="listitem"><p>The <code class="literal">image</code> to be used doing a reference to the image generated using <code class="literal">EIB</code> on the previous section (<a class="xref" href="atip-automated-provisioning.html#ztp-eib-edge-image" title="26.2. Generate the downstream Image using EIB">Section 26.2, “Generate the downstream Image using EIB”</a>) and the <code class="literal">checksum</code> and <code class="literal">checksumType</code> to be used to validate the image.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw</pre></div><p>The <code class="literal">Metal3DataTemplate</code> object to specify the <code class="literal">metaData</code> for the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>Once the file is created joining the previous blocks, the following command has to be executed in the management cluster to start provisioning the new bare metal host:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect1" id="ztp-multi-node" data-id-title="Downstream Cluster Provisioning with ZTP (multi-node)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.4 </span><span class="title-name">Downstream Cluster Provisioning with ZTP (multi-node)</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#ztp-multi-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section describes the workflow to automate the provisioning of a multi-node downstream cluster using <code class="literal">ZTP</code> and <code class="literal">MetalLB</code> as a load balancer strategy.
Basically, this is the simplest way to automate the provisioning of a downstream cluster. The following diagram shows the workflow to automate the provisioning of a multi-node downstream cluster using <code class="literal">ZTP</code> and <code class="literal">MetalLB</code>.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> as described in the previous section (<a class="xref" href="atip-automated-provisioning.html#ztp-eib-edge-image" title="26.2. Generate the downstream Image using EIB">Section 26.2, “Generate the downstream Image using EIB”</a>) with the minimal configuration to set up the downstream cluster has to be located in the management cluster exactly on the path you configured on this section (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>).</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. Please check the Management Cluster section for more information. <a class="xref" href="atip-management-cluster.html" title="Chapter 24. Setting up the Management Cluster">Chapter 24, <em>Setting up the Management Cluster</em></a></p></li></ul></div><p><span class="strong"><strong>Workflow</strong></span></p><p>The following diagram shows the workflow to automate the provisioning of a multi-node downstream cluster using <code class="literal">ZTP</code>:</p><div class="informalfigure"><div class="mediaobject"><a href="images/atip-automate-multinode1.png"><img src="images/atip-automate-multinode1.png" width="NaN" alt="atip automate multinode1" title="atip automate multinode1"/></a></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Enroll the  three bare metal Hosts to make them available for the provisioning process.</p></li><li class="listitem"><p>Provision the three bare metal Host to install and configure the operating system and the Kubernetes cluster using <code class="literal">MetalLB</code>.</p></li></ol></div><p><span class="strong"><strong>Enroll the bare metal Hosts</strong></span></p><p>The first step is to enroll the three bar metal hosts in the management cluster to make them available to be provisioned.
To do that, the following files (<code class="literal">bmh-example-node1.yaml</code>, <code class="literal">bmh-example-node2.yaml</code> and <code class="literal">bmh-example-node3.yaml</code>) have to be created in the management-cluster, to specify the <code class="literal">BMC</code> credentials to be used and the <code class="literal">BaremetalHost</code> object to be enrolled in the management cluster.</p><div id="id-1.7.8.5.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Only the values between <code class="literal">$\{…​\}</code> have to be replaced with the real values.</p></li><li class="listitem"><p>Only will be explained 1 host to make the process clear, but the same process has to be done for the other two nodes.</p></li></ul></div></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</pre></div><p>where:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${BMC_NODE1_USERNAME}</code> - The username for the BMC of the first bare metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_PASSWORD}</code> - The password for the BMC of the first bare metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_MAC}</code> - The MAC address of the first bare metal host to be used.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_ADDRESS}</code> - The URL for the first bare metal host BMC (e.g <code class="literal">redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</code>). If you want to know more about the different options available depending on your hardware provider, please check the following <a class="link" href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md" target="_blank">link</a>.</p></li></ul></div><p>Once the file is created, the following command has to be executed in the management cluster to start enrolling the bare metal hosts in the management cluster:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</pre></div><p>The new bare metal host objects will be enrolled changing their state from registering to inspecting and available. The changes can be checked using the following command:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get bmh -o wide</pre></div><div id="id-1.7.8.5.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">BaremetalHost</code> object will be in the <code class="literal">registering</code> state until the <code class="literal">BMC</code> credentials are validated. Once the credentials are validated, the <code class="literal">BaremetalHost</code> object will change its state to <code class="literal">inspecting</code>, and this step could take some time depending on the hardware (up to 20 minutes). During the inspecting phase, the hardware information is retrieved and the Kubernetes object is updated. You can check the information using the following command <code class="literal">kubectl get bmh -o yaml</code>.</p></div><p><span class="strong"><strong>Provision Step</strong></span></p><p>Once the three bar metal hosts are enrolled and available, the next step is to provision the bare metal hosts to install and configure the operating system and the Kubernetes cluster creating a load balancer to manage them.
To do that, the following file (<code class="literal">capi-provisioning-example.yaml</code>) has to be created in the management-cluster with the following information (the `capi-provisioning-example.yaml can be generated joining the following blocks).</p><div id="id-1.7.8.5.22" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Only the values between <code class="literal">$\{…​\}</code> have to be replaced with the real values.</p></li><li class="listitem"><p>The <code class="literal">VIP</code> address is a reserved IP address that is not assigned to any node and is used to configure the load balancer.</p></li></ul></div></div><p>The cluster definition where the cluster network can be configured using the <code class="literal">pods</code> and the <code class="literal">services</code> blocks. Also, it contains the references to the control plane and the infrastructure (using the <code class="literal">Metal<sup>3</sup></code> provider) objects to be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</pre></div><p>The <code class="literal">Metal3Cluster</code> object to specify the control plane endpoint which uses the <code class="literal">VIP</code> address already reserved (replacing the <code class="literal">${DOWNSTREAM_VIP_ADDRESS}</code>) to be configured and the <code class="literal">noCloudProvider</code> because the three bar metal nodes will be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS}
    port: 6443
  noCloudProvider: true</pre></div><p>The <code class="literal">RKE2ControlPlane</code> object to specify the control plane configuration to be used and the <code class="literal">Metal3MachineTemplate</code> object to specify the control plane image to be used.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The number of replicas to be used (in this case, 3)</p></li><li class="listitem"><p>The advertisement mode to be used by the Load Balancer (<code class="literal">address</code> will use the L2 implementation), as well as the address to be used (replacing the <code class="literal">${EDGE_VIP_ADDRESS}</code> with the <code class="literal">VIP</code> address).</p></li><li class="listitem"><p>The <code class="literal">serverConfig</code> with the <code class="literal">CNI</code> plugin to be used (in this case, <code class="literal">Cilium</code>), and the <code class="literal">tlsSan</code> to be used to configure the <code class="literal">VIP</code> address.</p></li><li class="listitem"><p>The agentConfig block contains the <code class="literal">Ignition</code> format to be used and the <code class="literal">additionalUserData</code> to be used to configure the <code class="literal">RKE2</code> node with some information like:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The systemd service named <code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.</p></li><li class="listitem"><p>The <code class="literal">storage</code> block which contains the helm charts to be used to install the <code class="literal">MetalLB</code> and the <code class="literal">endpoint-copier-operator</code>.</p></li><li class="listitem"><p>The <code class="literal">metalLB</code> custom resource file with the <code class="literal">IPaddressPool</code> and the <code class="literal">L2Advertisement</code> to be used (replacing the <code class="literal">${EDGE_VIP_ADDRESS}</code> with the <code class="literal">VIP</code> address).</p></li><li class="listitem"><p>The <code class="literal">endpoint-svc.yaml</code> file to be used to configure the <code class="literal">kubernetes-vip</code> service to be used by the <code class="literal">MetalLB</code> to manage the <code class="literal">VIP</code> address.</p></li></ul></div></li><li class="listitem"><p>The last block of information contains the Kubernetes version to be used. The <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (e.g. <code class="literal">v1.28.7+rke2r1</code>).</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  registrationMethod: "address"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS}
      - https://${EDGE_VIP_ADDRESS}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    repo: https://suse-edge.github.io/charts
                    chart: endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 0.2.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    repo: https://suse-edge.github.io/charts
                    chart: metallb
                    targetNamespace: metallb-system
                    version: 0.14.3
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS}/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "Node-multinode-cluster"</pre></div><p>The <code class="literal">Metal3MachineTemplate</code> object to specify the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">dataTemplate</code> to be used doing a reference to the template.</p></li><li class="listitem"><p>The <code class="literal">hostSelector</code> to be used matching with the label created during the enrollment process.</p></li><li class="listitem"><p>The <code class="literal">image</code> to be used doing a reference to the image generated using <code class="literal">EIB</code> on the previous section (<a class="xref" href="atip-automated-provisioning.html#ztp-eib-edge-image" title="26.2. Generate the downstream Image using EIB">Section 26.2, “Generate the downstream Image using EIB”</a>) and the <code class="literal">checksum</code> and <code class="literal">checksumType</code> to be used to validate the image.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw</pre></div><p>The <code class="literal">Metal3DataTemplate</code> object to specify the <code class="literal">metaData</code> for the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>Once the file is created joining the previous blocks, the following command has to be executed in the management cluster to start provisioning the new three bar metal hosts:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect1" id="ztp-add-advanced-network" data-id-title="Additional Features with ZTP - advanced network configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.5 </span><span class="title-name">Additional Features with ZTP - advanced network configuration</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#ztp-add-advanced-network">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">ZTP</code> workflow allows to automate the provisioning of the downstream clusters using advanced network configuration like DHCP-less, bond+vlans. The following sections describes the differences that can be used to automate the provisioning of the downstream clusters using advanced network configuration.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> has to include the network folder and the script following this section (<a class="xref" href="atip-automated-provisioning.html#ztp-add-network-eib">???TITLE???</a>)</p></li><li class="listitem"><p>The image generated using <code class="literal">EIB</code> as described in the previous section (<a class="xref" href="atip-automated-provisioning.html#ztp-eib-edge-image" title="26.2. Generate the downstream Image using EIB">Section 26.2, “Generate the downstream Image using EIB”</a>) has to be located in the management cluster exactly on the path you configured on this section (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>).</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. Please check the Management Cluster section for more information. <a class="xref" href="atip-management-cluster.html" title="Chapter 24. Setting up the Management Cluster">Chapter 24, <em>Setting up the Management Cluster</em></a></p></li></ul></div><p><span class="strong"><strong>Configuration</strong></span></p><p>Using the following two section as the base to enroll and provision the hosts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Downstream Cluster Provisioning with ZTP (single-node) (<a class="xref" href="atip-automated-provisioning.html#ztp-single-node" title="26.3. Downstream Cluster Provisioning with ZTP (single-node)">Section 26.3, “Downstream Cluster Provisioning with ZTP (single-node)”</a>)</p></li><li class="listitem"><p>Downstream Cluster Provisioning with ZTP (multi-node) (<a class="xref" href="atip-automated-provisioning.html#ztp-multi-node" title="26.4. Downstream Cluster Provisioning with ZTP (multi-node)">Section 26.4, “Downstream Cluster Provisioning with ZTP (multi-node)”</a>)</p></li></ul></div><p>The changes required to enable the advanced network configuration are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Enrollment step: The following new example file with a secret containing the information about the <code class="literal">networkData</code> to be used to configure for example, the static <code class="literal">IPs</code> and <code class="literal">VLAN</code> for the downstream cluster</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</pre></div><p>This file contains the <code class="literal">networkData</code> in a <code class="literal">nmstate</code> format to be used to configure the advance network configuration (e.g. <code class="literal">static IPs</code>, and <code class="literal">VLAN</code>) for the downstream cluster.
As you can see, the example shows the configuration to enable the interface with static IPs, as well as the configuration to enable the VLAN using the base interface.
Any other <code class="literal">nmstate</code> example could be defined to be used to configure the network for the downstream cluster in order to adapt to the specific requirements.
where the following variables have to be replaced with the real values:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${CONTROLPLANE1_INTERFACE}</code> - The control plane interface to be used for the edge cluster (e.g. <code class="literal">eth0</code>).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE1_IP}</code> - The IP address to be used as a endpoint for the edge cluster (should match with the kubeapi-server endpoint).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE1_PREFIX}</code> - The CIDR to be used for the edge cluster (e.g. <code class="literal">24</code> in case you want <code class="literal">/24</code> or <code class="literal">255.255.255.0</code>).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE1_GATEWAY}</code> - The gateway to be used for the edge cluster (e.g. <code class="literal">192.168.100.1</code>).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE1_MAC}</code> - The MAC address to be used for the control plane interface (e.g. <code class="literal">00:0c:29:3e:3e:3e</code>).</p></li><li class="listitem"><p><code class="literal">${DNS_SERVER}</code> - The DNS to be used for the edge cluster (e.g. <code class="literal">192.168.100.2</code>).</p></li><li class="listitem"><p><code class="literal">${VLAN_ID}</code> - The VLAN ID to be used for the edge cluster (e.g. <code class="literal">100</code>).</p></li></ul></div><p>Also, the reference to that secret using <code class="literal">preprovisioningNetworkDataName</code> is needed in the <code class="literal">BaremetalHost</code> object at the end of the file to be enrolled in the management cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: flexran-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</pre></div><div id="id-1.7.8.6.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>In case you need to deploy a multi-node cluster, the same process has to be done for the other nodes.</p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Provision step: The block of information related to the network-data has to be removed because the platform is including the network data configuration into the secret <code class="literal">controlplane-0-networkdata</code>.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><div id="id-1.7.8.6.18" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">Metal3DataTemplate</code> <code class="literal">networkData</code> and <code class="literal">Metal3 IPAM</code> is not currently supported, only the configuration via static secrets is fully supported.</p></div></section><section class="sect1" id="ztp-add-telco" data-id-title="Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.6 </span><span class="title-name">Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#ztp-add-telco">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">ZTP</code> workflow allows to automate the Telco features to be used in the downstream clusters in order to run Telco workloads on top of that servers.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> has to include the specific Telco packages following this section (<a class="xref" href="atip-automated-provisioning.html#ztp-add-telco-feature-eib">???TITLE???</a>)</p></li><li class="listitem"><p>The image generated using <code class="literal">EIB</code> as described in the previous section (<a class="xref" href="atip-automated-provisioning.html#ztp-eib-edge-image" title="26.2. Generate the downstream Image using EIB">Section 26.2, “Generate the downstream Image using EIB”</a>)  has to be located in the management cluster exactly on the path you configured on this section (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>).</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. Please check the Management Cluster section for more information. <a class="xref" href="atip-management-cluster.html" title="Chapter 24. Setting up the Management Cluster">Chapter 24, <em>Setting up the Management Cluster</em></a></p></li></ul></div><p><span class="strong"><strong>Configuration</strong></span></p><p>Using the following two section as the base to enroll and provision the hosts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Downstream Cluster Provisioning with ZTP (single-node) (<a class="xref" href="atip-automated-provisioning.html#ztp-single-node" title="26.3. Downstream Cluster Provisioning with ZTP (single-node)">Section 26.3, “Downstream Cluster Provisioning with ZTP (single-node)”</a>)</p></li><li class="listitem"><p>Downstream Cluster Provisioning with ZTP (multi-node) (<a class="xref" href="atip-automated-provisioning.html#ztp-multi-node" title="26.4. Downstream Cluster Provisioning with ZTP (multi-node)">Section 26.4, “Downstream Cluster Provisioning with ZTP (multi-node)”</a>)</p></li></ul></div><p>The Telco features which are covered on this section are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>DPDK and VFs creation</p></li><li class="listitem"><p>SR-IOV and VFs allocation to be used by the workloads</p></li><li class="listitem"><p>CPU isolation and performance tuning</p></li><li class="listitem"><p>Huge-pages configuration</p></li><li class="listitem"><p>Kernel parameters tuning</p></li></ul></div><div id="id-1.7.8.7.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the Telco features, please check <a class="xref" href="atip-features.html" title="Chapter 25. Telco Features Configuration">Chapter 25, <em>Telco Features Configuration</em></a></p></div><p>The changes required to enable the Telco features shown above are all inside the <code class="literal">RKE2ControlPlane</code> block in the provision file <code class="literal">capi-provisioning-example.yaml</code>. The rest of the information inside the file <code class="literal">capi-provisioning-example.yaml</code> are the same as the information covered on the provisioning section (<a class="xref" href="atip-automated-provisioning.html#ztp-single-node-provision">Section 26.3, “Downstream Cluster Provisioning with ZTP (single-node)”</a>).</p><p>To make the process clear, the changes required on that block (<code class="literal">RKE2ControlPlane</code>) to enable the Telco features are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">preRKE2Commands</code> to be used to execute the commands before the <code class="literal">RKE2</code> installation process. In this case, the <code class="literal">modprobe</code> command to enable the <code class="literal">vfio-pci</code> and the <code class="literal">SR-IOV</code> kernel modules.</p></li><li class="listitem"><p>The ignition file <code class="literal">/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</code> to be used in order to define the interfaces, driver, and number of <code class="literal">VFs</code> to be created and exposed to the workloads.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The values inside the configmap <code class="literal">sriov-custom-auto-config</code> are the only values to be replaced with the real values.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${RESOURCE_NAME1}</code> - The resource name to be used for the first <code class="literal">PF</code> interface (e.g. <code class="literal">sriov-resource-du1</code>). It will be added to the prefix <code class="literal">rancher.io</code> to be used as a label to be used by the workloads (e.g. <code class="literal">rancher.io/sriov-resource-du1</code>).</p></li><li class="listitem"><p><code class="literal">${SRIOV-NIC-NAME1}</code> - The name of the first <code class="literal">PF</code> interface to be used (e.g. <code class="literal">eth0</code>).</p></li><li class="listitem"><p><code class="literal">${PF_NAME1}</code> - The name of the first physical function <code class="literal">PF</code> to be used. You can generate more complex filters using this (e.g. <code class="literal">eth0#2-5</code>).</p></li><li class="listitem"><p><code class="literal">${DRIVER_NAME1}</code> - The driver name to be used for the first <code class="literal">VF</code> interface (e.g. <code class="literal">vfio-pci</code>).</p></li><li class="listitem"><p><code class="literal">${NUM_VFS1}</code> - The number of <code class="literal">VFs</code> to be created for the first <code class="literal">PF</code> interface (e.g. <code class="literal">8</code>).</p></li></ul></div></li></ul></div></li><li class="listitem"><p>The <code class="literal">/var/sriov-auto-filler.sh</code> to be used as a translator between the high-level configmap <code class="literal">sriov-custom-auto-config</code> and the <code class="literal">sriovnetworknodepolicy</code> which contains the low-level hardware information. This script has been created to abstract the user from the complexity to know in advance the hardware information. No changes are required in this file, but it should be present if we need to enable <code class="literal">sr-iov</code> and create <code class="literal">VFs</code>.</p></li><li class="listitem"><p>The kernel arguments to be used to enable the following features:</p></li></ul></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Parameter</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Value</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Description</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>isolcpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Isolate the cores 1-30 and 33-62</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>skew_tick</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to skew the timer interrupts across the isolated CPUs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the timer tick on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz_full</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>rcu_nocbs</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the RCU callbacks on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>kthread_cpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,31,32,63</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the kthreads on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>irqaffinity</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,31,32,63</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the interrupts on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>processor.max_cstate</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Prevents the CPU from dropping into a sleep state when idle</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>intel_idle.max_cstate</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Disables the intel_idle driver and allows acpi_idle to be used</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows to use vfio for the dpdk interfaces</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>intel_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Enables to use vfio for VFs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows to set the size of huge pages to 1G</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepages</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>40</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Number of hugepages defined before</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>default_hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; "><p>Default value to enable huge pages</p></td></tr></tbody></table></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The following systemd services in order to enable:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.</p></li><li class="listitem"><p>The <code class="literal">cpu-performance.service</code> to enable the CPU performance tuning. The <code class="literal">${CPU_FREQUENCY}</code> has to be replaced with the real values (e.g. <code class="literal">2500000</code> to set the CPU frequency to <code class="literal">2.5GHz</code>).</p></li><li class="listitem"><p>The <code class="literal">cpu-partitioning.service</code> to enable the isolation cores of <code class="literal">CPU</code> (e.g. <code class="literal">1-30,33-62</code>).</p></li><li class="listitem"><p>The <code class="literal">sriov-custom-auto-vfs.service</code> to install the <code class="literal">sriov</code> helm chart, wait until custom resources are created and run the <code class="literal">/var/sriov-auto-filler.sh</code> to replace the values in the config map <code class="literal">sriov-custom-auto-config</code> and create the <code class="literal">sriovnetworknodepolicy</code> to be used by the workloads.</p></li></ul></div></li><li class="listitem"><p>The <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (e.g. <code class="literal">v1.28.7+rke2r1</code>).</p></li></ul></div><p>With all these changes mentioned, the <code class="literal">RKE2ControlPlane</code> block in the <code class="literal">capi-provisioning-example.yaml</code> will look like the following:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/sriov-auto-filler.sh
              overwrite: true
              contents:
                inline: |
                  #!/bin/bash
                  cat &lt;&lt;- EOF &gt; /var/sriov-networkpolicy-template.yaml
                  apiVersion: sriovnetwork.openshift.io/v1
                  kind: SriovNetworkNodePolicy
                  metadata:
                    name: atip-RESOURCENAME
                    namespace: kube-system
                  spec:
                    nodeSelector:
                      feature.node.kubernetes.io/network-sriov.capable: "true"
                    resourceName: RESOURCENAME
                    deviceType: DRIVER
                    numVfs: NUMVF
                    mtu: 1500
                    nicSelector:
                      pfNames: ["PFNAMES"]
                      deviceID: "DEVICEID"
                      vendor: "VENDOR"
                      rootDevices:
                        - PCIADDRESS
                  EOF

                  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml; export KUBECTL=/var/lib/rancher/rke2/bin/kubectl
                  while [ $(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -ojson | jq -r '.items[].status.syncStatus') != "Succeeded" ]; do sleep 1; done
                  input=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get cm sriov-custom-auto-config -n kube-system -ojson | jq -r '.data."config.json"')
                  jq -c '.[]' &lt;&lt;&lt; $input | while read i; do
                    interface=$(echo $i | jq -r '.interface')
                    pfname=$(echo $i | jq -r '.pfname')
                    pciaddress=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.pciAddress")
                    vendor=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.vendor")
                    deviceid=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.deviceID")
                    resourceName=$(echo $i | jq -r '.resourceName')
                    driver=$(echo $i | jq -r '.driver')
                    sed -e "s/RESOURCENAME/$resourceName/g" \
                        -e "s/DRIVER/$driver/g" \
                        -e "s/PFNAMES/$pfname/g" \
                        -e "s/VENDOR/$vendor/g" \
                        -e "s/DEVICEID/$deviceid/g" \
                        -e "s/PCIADDRESS/$pciaddress/g" \
                        -e "s/NUMVF/$(echo $i | jq -r '.numVFsToCreate')/g" /var/sriov-networkpolicy-template.yaml &gt; /var/lib/rancher/rke2/server/manifests/$resourceName.yaml
                  done
              mode: 0755
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - intel_pstate=passive
            - processor.max_cstate=1
            - intel_idle.max_cstate=0
            - iommu=pt
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - kthread_cpus=${NON-ISOLATED_CPU_CORES}
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-performance.service
              enabled: true
              contents: |
                [Unit]
                Description=CPU perfomance
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "cpupower frequency-set -g performance; cpupower frequency-set -u ${CPU_FREQUENCY}; cpupower frequency-set -d ${CPU_FREQUENCY}"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SR-IOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
                ExecStartPost=/bin/sh -c "helm repo add suse-edge https://suse-edge.github.io/charts"
                ExecStartPost=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "helm --kubeconfig /etc/rancher/rke2/rke2.yaml install sriov-crd suse-edge/sriov-crd -n kube-system"
                ExecStartPost=/bin/sh -c "helm --kubeconfig /etc/rancher/rke2/rke2.yaml install sriov-network-operator suse-edge/sriov-network-operator -n kube-system"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/var/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</pre></div><p>Once the file is created joining the previous blocks, the following command has to be executed in the management cluster to start provisioning the new downstream cluster using the Telco features:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect1" id="ztp-private-registry" data-id-title="Additional Features with ZTP - Private Registry"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.7 </span><span class="title-name">Additional Features with ZTP - Private Registry</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#ztp-private-registry">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">ZTP</code> workflow allows to automate the provision of the downstream clusters and configure the private registry as a mirror to enable the images to be used by the workloads.</p><p>The first step to enable the private registry is to create the secret containing the information about the private registry to be used by the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</pre></div><p>The <code class="literal">tls.crt</code>, <code class="literal">tls.key</code>, and <code class="literal">ca.crt</code> are the certificates to be used to authenticate the private registry. The <code class="literal">username</code> and <code class="literal">password</code> are the credentials to be used to authenticate the private registry.</p><div id="id-1.7.8.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">tls.crt</code>, <code class="literal">tls.key</code>, <code class="literal">ca.crt</code> , <code class="literal">username</code> and <code class="literal">password</code> have to be encoded in base64 format before to be used in the secret.</p></div><p>With all these changes mentioned, the <code class="literal">RKE2ControlPlane</code> block in the <code class="literal">capi-provisioning-example.yaml</code> will look like the following:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</pre></div><p>Where the <code class="literal">registry.example.com</code> is the example name of the private registry to be used by the downstream cluster, and it should be replaced with the real values.</p></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="atip-features.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 25 </span>Telco Features Configuration</span></a> </div><div><a class="pagination-link next" href="atip-lifecycle.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 27 </span>Lifecycle Actions</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="atip-automated-provisioning.html#id-introduction-2"><span class="title-number">26.1 </span><span class="title-name">Introduction</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#ztp-eib-edge-image"><span class="title-number">26.2 </span><span class="title-name">Generate the downstream Image using EIB</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#ztp-single-node"><span class="title-number">26.3 </span><span class="title-name">Downstream Cluster Provisioning with ZTP (single-node)</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#ztp-multi-node"><span class="title-number">26.4 </span><span class="title-name">Downstream Cluster Provisioning with ZTP (multi-node)</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#ztp-add-advanced-network"><span class="title-number">26.5 </span><span class="title-name">Additional Features with ZTP - advanced network configuration</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#ztp-add-telco"><span class="title-number">26.6 </span><span class="title-name">Additional Features with ZTP - Telco Features (DPDK, SR-IOV, CPU isolation, Huge-pages, NUMA, etc.)</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#ztp-private-registry"><span class="title-number">26.7 </span><span class="title-name">Additional Features with ZTP - Private Registry</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>